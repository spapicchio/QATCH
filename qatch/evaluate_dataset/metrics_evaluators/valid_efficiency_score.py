from __future__ import annotations

import time

import numpy as np
from func_timeout import FunctionTimedOut

from .base_evaluator import BaseEvaluator
from .execution_accuracy import ExecutionAccuracy
from ...connectors import BaseConnector


def _remove_outliers(array: list[float]) -> list[float]:
    mean, std = np.mean(array), np.std(array)
    lower_bound = mean - 3 * std
    upper_bound = mean + 3 * std
    return [x for x in array if lower_bound <= x <= upper_bound]


class ValidEfficiencyScore(BaseEvaluator):
    @property
    def metric_name(self):
        return 'valid_efficiency_score'

    def run_metric(self,
                   target: list[list],
                   prediction: list[list],
                   *args, **kwargs) -> float | int:
        """
        Execute the ValidEfficiencyScore metric.
        Credit to the logic:
        "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs"

        VES is calculated by multiplying the execution accuracy with the relative execution efficiency.
        This score is calculated as the ratio between the expected value of the target,
        divided by the expected value of the prediction under square root.
        the expected value is calculated for 100 run of the query on CPU.
        The outliers outside 3 standard deviations interval are removed.

        Notes:
            - The VES is between [0, +infinite). Larger is the metric, faster is the prediction
            - If the prediction have an execution accuracy=0 also the VES is 0

        Args:
            target (list[list]): The expected output to which the prediction will be compared.
            prediction (list[list]): The output generated by prediction process.
            *args: Variable length argument list.
            **kwargs:
              - connector: A BaseConnector element which provides the necessary interfaces to the database.
              - predicted_query (str): A SQL query string that is predicted by the program.
              - target_query (str): The actual SQL query string.

        Returns:
            float | int: A score representing the measure between target output and the prediction.
             It's a product of execution accuracy and efficiency score. it is between 0 and + infinite. larger is better
        """

        connector = kwargs.get('connector')
        predicted_query = kwargs.get('predicted_query')
        target_query = kwargs.get('target_query')
        execution_accuracy = ExecutionAccuracy().run_metric(target, prediction)
        relative_efficiency_score = self.relative_execution_efficiency(predicted_query, target_query, connector)
        return execution_accuracy * relative_efficiency_score

    def relative_execution_efficiency(self, target_query: str, predicted_query: str, connector: BaseConnector) -> float:
        expected_time_target = self.calculate_expected_execution_time(target_query, connector)
        expected_time_prediction = self.calculate_expected_execution_time(predicted_query, connector)
        ratio = expected_time_target / expected_time_prediction
        return np.sqrt(ratio)

    def calculate_expected_execution_time(self, query: str, connector) -> float:
        times = []
        for _ in range(100):
            try:
                start_time = time.time()
                _ = connector.run_query(query)
                exec_time = time.time() - start_time
                times.append(exec_time)
            except FunctionTimedOut as e:
                # the timeout set in the connector
                times.append(e.timedOutAfter)
        times = _remove_outliers(times)
        expected_time = np.mean(times)
        return expected_time
